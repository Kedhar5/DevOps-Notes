Kubernetes/K8s
----------------
K8s is a "open source Container Orchestration software" tool 
Responsibilities include Container deployment, Scaling Descaling of containers & Load balancing 
K8s can be considered as an alternative for Docker Swarm 
K8s is written in GoLang and we can use CLI for interacting with Kubernetes 

Current stable version of Kubernetes is v1.29 
https://kubernetes.io/ 
https://kubernetes.io/docs/home/

Advantages of K8s
------------------
* Schedule the containers automatically 
* Self healing capabilities 
* Automated rollout & rollback 
* Horizontal scaling and Load Balancing 
* Service Discovery
* Storage Orchestration 

K8s Architecture 
-----------------
K8s is implemented in a cluster computing background 
Cluster can be created with one or more nodes as managers and rest of them as workers 
We can set up the clusters anywhere locally or on cloud 

In each and every node in a cluster we need to have a container runtime 
Container Runtime is a software to create and manage the containers 
Node can be a physical machine or Virtual machine 
We need to install Container runtime in each node present in the cluster 
Docker is one example for Container runtime software 

K8s APi resources/Objects 
---------------------------
Pods 
Deployments 
Services 
ConfigMap 
Secret 
Replica set 
Replication Controller 

API server is the frontend for Kubernetes 
API servers will interact with K8s APIs 
Kubectl is the CLI for K8s 

Kubectl will make API calls for Control/Master pane node 
We can use Kubectl, GUI & Client Libraries to interact with control pane node 

ETCD --> Distributed database of K8s, all the cluster values will be stored here 
Some of the data stored is Job scheduling information, Pods, State info etc..

Scheduler --> Repsonsible for scheduling the pods on nodes on the cluster 

Kubelet --> node agent that ensures the health of the container 

Kubeproxy --> helps us to have network proxy and load balancer in the same worker node 


Types of Clusters 
------------------
1.Self Managed K8s clusters 
Individual needs to manage the K8s cluster here 
We need to install servers, services, installations etc.. 
Automatic creation of nodes and deletion of unhealthy node is not possible in this scenario 
	a.Single Node/Local K8s cluster 
	Only one node will be present here 
	This node will act as both Master and Worker 
	This is mostly used for local developmental purposes
	
	b.Multi Node K8s Cluster
	Here Master will be seperate and Worker will be seperate 
	

2.Managed(Fully) K8s clusters --> Provided by Cloud service providers 
These are highly available and scalable 
Maximum K8s will recreate pods into someother nodes 
EKS --> Elastic Kubernetes Service (AWS Cloud) 

Self Managed K8s Cluster using kubeadm 
---------------------------------------
Master --> t2.medium --> Open all traffic for particular range 
2 Workers --> t2.micro --> Open the worker ports 

Use the Installation file to follow the commands 

Kubernetes Objects / K8s API Resources 
---------------------------------------
These are the entities of the K8s system . k8s will use this entities to represent the state of your cluster 

Node
NameSpace
Pod
Service 
ReplicationController
ReplicaSet
DaemonSet
Deployment 
StatefulSet 
HorizontalPodAutoScaler

Service 
Ingress 
NetworkPolicies 

PersistentVolume
PersistentVolumeClaim
StorageClasses

ConfigMap
Secret 

Role
RoleBinding
ClusterRole 
ClusterRoleBinding

The above are some of the entities in K8s

kubectl api-resources --> resources present in the K8s system 
kubectl get nodes --> gets the details of all the nodes present in the K8s system 
kubectl get namespaces 
kubectl get pods 

In K8s we cant directly create a container they are managed by pods 
Pod --> group of one or more containers 
Node --> Node is a system or a server part of a cluster 
		 Node can be a Master or Worker 
Workload --> An application running in a K8s cluster 
NameSpace --> Logical grouping of K8s resources in the cluster 
	We can have so many NameSpace in a single Cluster 
	This will be helpful when multiple teams are using the same cluster 
	namespace is a way of isolating group of resources within a single clusetr 
	resource name should be unique in a single namsepace 
	Any number of namespaces can be created within a cluster, each logically isolated from one other 
	Namespaces cannot be nested in each other 
	
	There are 4 namespaces which are created by default 
	kubectl get namespaces 
	kubectl get ns 
	By using the above commands we will get the namespaces in the system 
	when multiple teams are using the same cluster then we can use namespace 
	Types of namespaces
	--------------------
	default --> default namespace for objects which dont have defined namespace 
	kube-system --> for objects created by Kubernetes system 
		kubectl get all -n kube-system --> here kube-system can be replaced by any namespace name 
		We can see all the objects of the particular namespace using above command 
		Our objects should not be stored in kube-system namespace 
	kube-public --> default namespace for readability of clients which doesnot require any authentication 
	kube-node-lease --> namspace for objects related to cluster scaling
When should we create NameSpaces?
----------------------------------
Isolation --> Isolate resources from one team to other 
	Work in one ns will not affect the other 
Permissions --> Seperate permissions can be create for every namespace 
	We can protect sensitive data using these 
Resource Control --> defining the resource limit at namespace level 
	We can ensure each namespace can have access to certain amount of CPU and memory 
Performance --> APi will have less items to search through when we perform speific operations 

NameSpace creation 
--------------------
Imperative way 
----------------
kubectl create namespace test-ns --> NameSpace test-ns will be created 
kubectl label namespace test-ns team=testing 

kubectl run <podName> --image=<image> --port=<containerPort> -n test-ns 
ex:-- kubectl run nginxpod --image=nginx --port=80 -n test-ns 

Declarative way 
-----------------
testns.yaml  --> created in Stolkholm server 
-------------
apiVersion: v1
kind: Namespace
metadata:
  name: test-ns 
  labels: 
    team: testing 

kubectl create -f <filename>.yaml --> creates the resource if not exists 
kubectl apply -f <filename>.yaml --> it can create if resource dont exist and update if any changes 

kubectl describe <object> <objectName> --> Gives the entire data about the Object 
kubectl get all -A or kubectl get all --all-namespaces --> get all details of namespaces 

Pod
----
Pod is the smallest object we can create in Kubernetes 
Pod represent the running process it has conatiners in it 
We can have single or Multi container pods in Clusters 
Each pod has a unique IP address in the cluster 

In K8s we cant directly create a container or manage it so we have a concept called pod in K8s 
Pod is a group one or more containers which will run on the same node 
If a pod has multiple containers they will be related as main container and Helper containers 
In K8s we can attach Volumes to the pods 
Pods abstract network and storage away from the underlying container 

Pod Model types 
-------------------
Mostly a pod has only one conatiner, There are some instances where we can have multiple conatiners in the same pod 
There are two model pods we can create 
one conatiner per pod 
----------------------
Since pod is the smallest object it will manage the pod instead of managing the container 
POD is the "wrapper" for single container 

Multi-container-pod or Sidecar Containers 
------------------------------------------
A pod which can hold multiple containers in the same pod 
here Network and Storage will be same for both containers 
pod IP's are dynamic they are not same everytime 

creating a pod 
-----------------
imperative way 
---------------
kubectl run <podName> --image=<image> --port=<ContainerPort> 

declarative way --> creation done in server 
----------------
apiVersion: v1
kind: Pod 
metadata:
  name: nginxpod 
  labels:
    app: nginx
spec:
  containers:
  - name: nginxapp
    image: nginx 
	ports:
	- containerPort: 80
	
kubectl get pods -o wide --> View complete details of the pod 
kubectl describe pod <podName> --> complete details of the pod 


Labels 
-------
one object uses other in K8s Labels are used 
This is a key value pair

labels:
  app:ngnix 
  manager:Kedhar

kubectl run nginxpod --image=<image> --port=80 --labels="app=nginx"

declarative way 
---------------
apiVersion: v1
kind: Pod 
metadata:
  name: <podName>
  labels:
    <key>: <value>
  namespace: <test-ns>
spec:
  containers:
  - name: <nameOfContainer>
    image: <image>
    ports:
    - containerPort: <portNumber>
	env:
	- Name= Value
	resources:
	  requests:
	    cpu: ValueAvailable
		memory: valueavailable 
	  limits:
	    cpu: valueavailable
		memory: valueavailable
	Volumes:
	- name: <volumeName>
	  hostPath:
	    path: /test 
  
Selectors 
----------
These use the Label key to find a collection of objects matched with same value 

One pod in K8s cluster can communicate with other using the pod IP.  But this is not recommended for inter pod communication 
Here we can use one more object called Service 

Service 
--------
K8s object that is responsible for making the pod discoverable.
A service identifies the pod using the selectors. The application labels can be used as selectors here 
K8s will have DNs pods and it will be mapped to Service IP's 
Service is a logical concept real work is done by Kube-proxy 
Ex:--

apiVersion: v1 
kind: Service 
metadata: 
  name: <serviceName> 
  nameSpace: <nameSpace>
spec:
  type: clusterIP
  selector: 
    <key>: <value>
  ports:
  - port: <servicePort>
    targetPort: <containerPort>

kubectl get pods -o wide --show-labels --> to get the pod labels 
The endpoints in service is nothing but the pod IP's.
If there are multiple ep's then we can load balancing between them too
Now we can go to other pod and we can communicate to the service as follows 
	http://<serviceName>:port 
If pods from different namespace need to cok=mmunicate we use FQDL --> Fully Qualified domain name 
ex:-- http://<pod>.<nameSpace>.svc.cluster.local:port 

NodePort 
---------
Along with port, target port we can use NodePort 
Nodeport is used for external access 
the renage of NodePort is 30000-32767 
From outside the cluster we can access using the Ip and Node port 
If we donot mention the Nodeport it will be automatically assigned by K8s 
Instead of using clusterIP we can change the type as NodePort 

kubectl logs <podname> --> logs of the mentioned pod
We can multiple containers in the pods 
We can mention that in the pod spec 
We cannot add or update the pods directly 
We need to delete it 1st and create again 
kubectl delete pod <podName> 

Pod Lifecycle 
----------------
make a Pod request to API server 
API server saves the info for pod in ETCD 
Scheduler find the pod and schedules in the node 
Kubelet running on node sees the scheduled pod and starts container run time 
Container run time pulls the image and runs the conatiners for the pod 
Entire lifecycle can be stored in ETCD 

Kubernetes we should not create the pod directly since we can face single pont of failure 
Also there is a chance of pod not able to accomodate requests 

Static pod 
------------
Pods which are managed by the kubelet 
Static pods are not controlled by replication controller, Replica set controller etc..
Main use of of this is to run control plane components
Each Static pod has Ip as its suffix 
/etc/kubernetes/manifests 
We can just place the .yaml file in above directory and the static pod will be created 
we can remove the file whenever so that we can delete the static pod 

ReplicationController
------------
In K8s we shouldn't create pods directlhy we need to use Replication Controller
These are responsible for managing the pod life cycle 
It will allow us to create multiple pods with same configurations 
If something happens to one pod it will be created again using the template we have given 
This will also use container labels as selectors 
template will contain the metadata and spec fields of the pod that need to be created 
EX:--

apiVersion: v1 
kind: ReplicationController
metadata:
  name: <rcName>
spec:
  replicas: <no of pods>
  selector:
    <key>: <value> 
  template:
    metadata:
	  name: <podName> 
	  labels: 
	    <key>: <value>
	spec:
	  container:
	    - name: <containerName>
		  image: <image>
	  ports:
	  - containerPort: <portNumber>
	

If the pod needs to be deleted from the cluster then we need to delete the ReplicationConroller
Since if we delete the pod it will be created again we can delete the controller of the Pods 
or else we can use 
kubectl delete -f yamlFile --> All the resources defined in the file will be deleted using this 
We can delete the rc as below 
kubectl delete rc <RCName> -n <nameSpace> --> Delete the rc 

ReplicaSet 
------------
Similar to rc the difference is the selectors support 
It will also create and manage the pods. Here using selectors is mandatory 
This is like advanced version of replicationCntroller 
apiVersion is apps/v1 for replicaSet 
shortForm is rs 
selectors:
  matchLabels:
    <key>: <value> 

EX:--

apiVersion: apps/v1 
kind: ReplicaSet 
metadata:
  name: <rsName>
spec:
  replicas: <no of pods>
  selector:
    matchLabels:
      <key>: <value> 
  template:
    metadata:
	  name: <podName> 
	  labels: 
	    <key>: <value>
	spec:
	  container:
	    - name: <containerName>
		  image: <image>
	  ports:
	  - containerPort: <portNumber>
	  
DaemonSet 
-----------
This is a K8s object which make sure each and every node has a copy of the pod 
Deleting the DaemonSet will delete the pods it has created 
Applications which are agent based can be deployed as a DaemonSet
ex:

apiVersion: apps/v1 
kind: DaemonSet
metadata:
  name: <dsName>
  namespace: <NameSpace>
spec:
  selector:
    matchLabels:
	  <key>: <value>
  template:
    metadata:
	  labels:
	    <key>: <value>
	spec:
	  containers:
	  - name: <containerName>
	    image: <image>
		ports:
		- containerPort: <containerPort>
		
in Spec: in template: we can give the tolerations:
in this we can give key: effect: and operator:
In effect if we give exists then a copy of the pod will be created in Master plane too

Deployment
------------
It is also a K8s object which is the recommended way to deploy the application in K8s 
There are additional features in this objects 
Here we can replace old pods with new pods 
If we are fine with our app being down while the updates we can still use rc and rs 

Advantages 
-----------
This will create a ReplicaSet internally and rs will manage the pods 
We can update the Pod Template here 
There won't be any outage if we use this deployment object 
There will be 0 downtime deployment 
We can rollback to previous version using this object 

Deployment Strategies
----------------------
There are several kind of this strategies 

a.Rolling deployment 
------------------
This is the default deployment strategy in K8s 
It will work slowly, one by one 
It will replace the older pods with new pods without any downtime 
This will wait for new pods to be ready before it scale down the old one 

b.Recreate
---------
All of the old pods will be deleted first then the new pods will be created 
All the old pods will be deleted at once 
There will be downtime using this strategy 

ex:--
apiVersion: apps/v1 
kind: Deployment 
metadata:
  name: <DeploymentName>
spec:
  replicas: <no of pods>
  strategy:
    type: Recreate 
  selector:
    matchLabels:
	  <key>: <value>
  template:
    metadata:
	  labels:
	    <key>: <value>
	spec:
	  containers:
	  - name: <containerName>
	    image: <image>
		ports:
		- containerPort: <containerPort>

kubectl rollout status deployment <name> -n <nameSpace> 
kubectl rollout history deployment <name> --revision=<RevNo>

kubectl rollout undo deployment <name> -n <nameSpace> --to-revision <RevisionNumber>
if we dont mention --to-revision it will go back 1 revision 


Rolling Updates 
Need to be written 

Resource Requests and Limits 
Need to be written 

HPA --> Horizontal Pod Auto Scaler 
-----------------------------------
Automatically scales the number of pods in rc, replica set 
It will be implemented as a K8s resource. It will periodically adjust the no of pods 
By default it will observe for every 15 seconds 
HPA internally requires a metric 

If the load decreases then the no of pods will also be decreased 
By default metrics is not available in K8s 
HPA will interact with Metric API so we need to deploy an application called Metric Server 
Without metric server deployed in K8s server HPA cannot increase or decrease the pods 

Metric server is only for Auto scaling purpose 
Metric server is used for CPU or memory based Auto scaling 

kubectl top pods --> example command for finding whether Metric Server is installed or not 
ex:-- 

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler 
metadata:
  name: <HPA name>
  namespace: <namespace>
spec:
  scaleTargetref:
    apiVersion: apps/v1
	kind: Deployment 
	name: <deploymentName>
  minReplicas: 2
  maxreplicas: 4
  metrics:
  - type: Resource 
    resource:
	  name: cpu 
	  target:
	    type: Utilization
		averageutilization: <valueInPercentage>
  - type: Resource 
    resource:
	  name: memory  
	  target:
	    type: Utilization
		averageUtilization: <valueInPercentage>

we need to add the arugement --kubelet-insecure-tls in the yaml file and after that we can apply the file 
Even though if we define replicas in Deployments it will be overridden by the minReplicas in HPA 

Statefull 
----------
Here the application can be inside the cluster and Database is outside the cluster 
The application will be able to communicate still 
If both the app and DB are deployed as pods in K8s cluster then they can have communication between them 
This is called Inter pod communication and using the service we can make it possible 

ex:--
We can use sample mongo image from docker hub 
For DB we should not have more than 1 DB 

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: mongodb
  namespace: testns
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
      - name: mongo
        image: mongo
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb
---
---
apiVersion: v1
kind: Service
metadata:
  name: mongosvc
  namespace: testns
spec:
  type: ClusterIP
  selector:
    app: mongo
  ports:
  - port: 27017



Spring app Deployment 
----------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: springapp
  namespace: testns
spec:
  replicas: 2
  selector:
   matchLabels:
     app: springapp
  template:
    metadata:
      labels:
        app: springapp
    spec:
      containers:
      - name: springapp
        image: dockerhandson/spring-boot-app:1
        ports:
        - containerPort: 8080
        env:
        - name: MONGO_DB_HOSTNAME
          value: mongodbsvc
        - name: MONGO_INITDB_ROOT_USERNAME
          value: devdb
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: devdb
---
apiVersion: v1
kind: Service
metadata:
  name: springappsvc
  namespace: testns
spec:
  type: NodePort
  selector:
    app: springapp
  ports:
  - port: 80
    targetPort: 8080 

Volumes in K8s 
----------------
K8s supports many number of volumes and a pod can use any number of volumes 
The way we define container: in pod template we can mention volume: in same way too 

Voulme Types:
  hostPath 
  emptyDir
  nfs 
  azureDisk
  azureFile
  googlePersistentDisk
  awsElasticBlockStore
  glusterFs
  ceaph
  configMap
  secret
  persistentVolumeClaim

by default awsElasticBlockStore is not supported in K8s now it is changed in v1.29 

hostPath Volume
----------------
System wherever the pods are installed 
It has many risks too, because we can access shell from the system 
recommended to mount read only volumes 

If we have multi container pod we can use volume for multiple containers or we can attach multiple volumes too 

EmptyDir Volume 
----------------
It is initially empty, When a pod is gonna be created and it is assigned to the node 
Pod is removed from the node then the data will be deleted permanently 
If a container is crashed then the data remains 
It is just like a temp storage, It is not recommended to use this kind of volumes 


session 15 need to be written 

pv gets associated with pvc based on access modes and capacity requested 
pvc is used as a volume for a pod, Even though application is deleted pvc will be present 
the volume details will be permanent even after application deletion 

ACCESS MODES 
--------------
ReadWriteOnce (RWO)
  Volume can be mounted by Read write by a single node 
  If multiple pods are scheduled in same node then also this mode can be used 
ReadWriteMany(RWX)
  Volume can mounted as Read Write by many nodes 
  Pods scheduled on different nodes can use this volume as shared volume 
ReadOnlyMany(ROX)
  Pods can read even if they are in any node in cluster 
  This is supported for only CSI volumes 

All the volumes wont support all access modes 
We need to use the mode as per our requirement 

ReClaim Policies 
-----------------
If we are done with app completely and need not deploy it again so we can delete the pvc 
If pods are still running we cannot delete the associated pvc 
If the pvc is deleted what happens to PV depands on the ReClaim policy

Retain 
  Allows manual reclaimation of a resource 
  Volume will not be allowed for other claim
  Volume and the data in it wont be deleted   
Recycle
  PV will not be deleted here and it will be available for a new claim 
Delete
  Here deletion removes both pvc and the associated volume 
  
If the PV is deleted and not present even if we request using pvc storage will not be provides 

Storage Class(sc)
------------------
It is also a K8s object it has paramters like provisioner, paraameter & ReClaimpolicy 
If we configure multiple storage classes for a claim as a K8s admin we can use 1 class as default 
K8s dont have any nfs provisioner by default 
We can use external provisioner to create sc 

NFS Subdir External Provisioner uses already configured nfs server for dynamically creating the PV 
For each PV it will create a sub directory in the nfs server 
We need to deploy the external nfs provisioner and using that we need to configure the storage class 
If there are multiple storage class we need to mention which storage class we need to use 

Now since we have configured the sc even if we just create the pvc it will be allocated a storage/pv 


ConfigMap & Secret 
-------------------
App configurations can change between the environments and we want our app to be portable 
We need to make sure that our image can be usd for all the environments 
K8s will allow us to provide confif info to our application through ConfigMap & Secret

We can store data as key value pair of data in ConfigMap 
In ConfigMap we use non confidential data 
We can use Secret to store confidential data 
We can keep config data seperate from your application code 
We can mount ConfigMap as volumes also 

apiVersion: v1 
kind: ConfigMap
metadata: 
  name: <configName>
  namespace: <nameSpace>
data:
  # property like values 
  <key>: <value>
  
  # file like values 
  <FileName>:
    <key>: <value> 

in etcd K8s will maintain all the data 
Data can be passed to our app in 4 ways 
Data can be read as CLI or as volumes from ConfigMap

data can be read from ConfigMap as follows 
env:
  - name: <ENV var name>
    valueFrom:
	  ConfigMapKeyRef:
	    name: game-demo #ConfigMap name
		key: <env_variable> #Key to fetcgh from config map 

volumeMounts:
- name: config 
  mountPath: "/config"
  readOnly: true 

Use reference files from Kubernetes official website 

apiVersion: v1
kind: ConfigMap
metadata:
  name: tomcatconfig
  namespace: testns
data:
  tomcat-users.xml: #This is the file name 
    Conf ditectory data in TOmcat can be pasted here 
---
Java Web app yaml here
containers:
***
***
  volumeMounts:
  - name: tomcatconf
    mountPath: /usr/local/tomcat/conf/tomcat-users.xml 
	subPath: "tomcat-users.xml"
volumes:
- name: <volume Name>
  configMap:
    name: tomcatconfig
	items:
	- key: "tomcat-users.xml"
	  path: "tomcat-users.xml"
	  
Secret
---------
It is a object in K8s which contains smal amount of confidential data 
Secrets can be used in pod containers 
These are stored in etcd and tehy are not encrypted 

There are multile types of secrets in K8s 

apiVersion: v1
kind: Secret
metadata:
  name: springappsecret
  namespace: testns
type: opaque
data:
  mongodb_password: #base 64 value of password
  
password can be referred in spring app as below 
valueFrom:
  secretKeyRef:
    name: springappsecret #Secret name 
	key: mongodb_password #key in the secret file 