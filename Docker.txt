Docker
-------
Docker is a containerization tool.
Using Docker we can build,ship, run any processas a container 

Container --> Conatiner has everything which is required for your application or Software 
here everything means required environments + Softwares + Configurations + Your Code 

Using Docker we can build a package(Image) which can have softwares + Configurations + Code + Dependencies 
We can ship the package from one server to other server and we can deploy the app as a container 

Containerization Deployment 
-----------------------------
Physical/Virtual Server --> OS --> Containerization tool --> Containers 
Each container is isolated from other, Containers are fast and light weight 
Containers will not have OS they just have OS libraries to interact with Kernel 
Docker helps us to deploy multiple apps in same OS by isolating the all the process 
Docker helps us to isolate the Containers at the OS level 

In order to create a conatiner we need to create a container Image 
We can store these container in Artifactory repositorys 

Docker 2 Notes yet to be written

Docker Installation
----------------------
sudo apt update -y
sudo apt install docker.io -y
Even after installation we donot have permission to use docker so we add ubuntu user to docker dameon 
sudo usermod -aG docker ubuntu

Now docker is installed successfully
/var/lib/docker is the home directory 

Architecture 
----------------
Docker daemon --> docker s/w process that listens for docker API request 
It manages images conatiners networks volumes etc..

Docker client (CLI) --> Primary way where we can interact with docker 
When we use certain commands like docker run the client sends these commands to dockerd.
Docker CLI uses Docker API internally 

When we install docker it will have both Docker CLI and DockerDaemon 
We can have only Docker CLi outside the system and still we can interact with remote Docker host 

Docker registery --> These are artifactory repository to store artifacts or Images 
Docker registery has Docker repository and the repository holds the Images 
These can be hosted by a third party application too or we can set up for ourself 

Docker Hub is the community or public registery 

<registery Server/repo:tag>
ex--> nexus.tcs.com/Tomcat:1.0

If we donot mention tag it will pull the latest version of the tag 
In docker hub we have multiple images this is created and managed by Docker community 
If we donot mention the registery then also the address will reach Docker hub 

So for example if we create a container using a Tomcat tag then Tomcat will be running in our container 

Docker Flow
------------
Docker file is a normal text file which has instructions to create a Docker Image 
The Image will be created and then it can be stored in Docker Registery 
In Docker Hub we can create a Registery but only Public repository can be created 
So any one can pull the images from the public repository 
But for own apps we need to store the images in private repositories so we can use 3rd party apps like Nexus JFrog 
Here Image is nothing but a package 

One Docker Image is based on other image with additional customization 
We can build our own image by using Tomcat image as a base 
To Build our own in=mage we need to use Docker file which has all the instructions 
We can Create Move Start Delete the containers Using Docker CLI or API 

Docker network is used to establish the connection between docker containers and outside world usinf Docker Daemon 

docker ps and docker container ls are use used to get the existing containers 

To pull the container from a registery 
docker pull <image>
docker pull <registeryserver/repoName:tag>

docker pull nginx --> This will pull latest version of nginx from the Docker Hub 

If we use docker create it will juest create the container 
If we use docker run it will both create and run the container 

docker create --name <containername> -p <hostPort:containerPort> <image>
here -p is called port binding 

One container in docker can communicate ith other process this is called inter process communication

docker run -d --name <containername> -p <hostPort:containerPort> <image>
we already have a nginx image in our host so using docker run e can now create a Container 
here -d is used because if we use it our conatiner will start in detached mode 

docker run -d --name nginxapp -p 80:80 nginx 
docker inspect <containerName> 
this will give multiple info about the container 
curl -v 172.17.0.2:80 --> we can access the conatiner using the command as this 

We need to open the host port in AWS when we need to use the web application 

hub.docker.com --> Here we can create a account for the registery 
after creating the account we can create the repositories in docker hub 

This is the repo we have created in Docker Hub 
kedhar5/mavenwebapplication

When we build image it will be as 
docker build -t <username/reponame:tag>

for application it will be as 
<registeryEndPoint/repoName:tag>
nexus.tcs.com/repoName:tag 

Dockerfile 
-------------
Docker file is a simple text file with instructions 

FROM tomcat:8.0.21-jre8
COPY target/maven-web-application.war /usr/local/tomcat/webapps/maven-web-application.war

The above Dockerfile indicates that we are getting the Tomcat base image from Docker hub initially 
then the war file which is present in target directory will be copied to the webapps directory 

So FROM instruction indicates what is the parent image we need to use, So we are using Tomcat image which is built by Docker community \
/usr/local/tomcat we can get this path from the documentation of the image in Docker Hub 

docker build -t <image> <buildcontext>
build conetxt is set of files located at a path 
build command expects Dockerfile to be present in the directory 
docker build -t kedhar5/mavenwebapplication:1 .
Here . represents the current folder or context path 

1st we need to install maven to create the package 
sudo apt install maven --> used to install maven 
then we use mvn clean package to create the package 
then we can use docker build to build the image 

ubuntu@ip-172-31-0-208:~/maven-web-application$ docker build -t kedhar5/mavenwebapplication:1 .
DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
            Install the buildx component to build images with BuildKit:
            https://docs.docker.com/go/buildx/

Sending build context to Docker daemon  11.77MB
Step 1/2 : FROM tomcat:8.0.21-jre8
 ---> a0b976f32c18
Step 2/2 : COPY target/maven-web-application.war /usr/local/tomcat/webapps/maven-web-application.war
 ---> cf03a1e3c108
Successfully built cf03a1e3c108
Successfully tagged kedhar5/mavenwebapplication:1

docker history kedhar5/mavenwebapplication:1
We can use the above command to verify the history of a particular image 

docker push kedhar5/mavenwebapplication:1
This is used to push the image to our repo in docker hub
But initially it will be denied so we need to provide access as below 

docker login -u <username> -p <password> </registery endpoint>
if we use docker hub no need to use registery end point 

docker login -u kedhar5 -p Kedhar@464
docker push kedhar5/mavenwebapplication:1 --> Now we can use this so that image will be pushed to our repo 

So the Whole process as follows 
Clone the code from GitHub 
Create a dockerhub account and create a repository 
create a docker file and give the instructions in it
create the war file using maven build tool 
use docker build to build the image 
use docker login to login to the registery 
use docker push to push the image to the registery 

Now that the image is created we can deploy that in a different server also 
we can use docker run to create and deploy the application 
docker run --name mavenwebapp -d -p 8080:8080 kedhar5/mavenwebapplication:1
docker inspect mavenwebapp --> Used to get the container IP 
We can use container Ip to access the application internally 

To access the application we have created on the image we need to use host Ip and Host port 

So to summarize we have pulled a container from a public registery and we user docker run command to run the conatiner 
We have open port 8080 in AWS and then we have accessed the application 
All of this was done very raipdly 

Image commands 
----------------
Image is a package with everything that are required to run your application 

docker images --> How many images are there in our local system 
docker images -q --> displays only image ID's

docker pull <image> --> pull image from a registery end point 
	docker pull nexus.com/mavenapp:1

docker login --username <username> --password <password> <registeryServer>
docker login -u <username -p <password> <registeryServer>
	docker login -u admin -p admin@5 nexus.com 

docker help --> lists out all the options for commands 

docker history <image> --> Lists out the layers of the docker image 

docker build -t <image> <buildContext>
	docker build -t kedhar5/mavenwebapp:1 .
	Here . represent the current working directory 
	Here this also expects the Dockerfile 
	We can also use a different file instead of Dockerfile then 
	docker build -f <filename> -t <image>

docker push <image>
	docker push kedhar5/mavenwebapp:1

docker inspect <imageID or imageTag> --> details of the image 

docker rmi <imageID> or docker image rm <imageID> --> delete the image in the local host 

docker rmi imageID1 ImageID2 --> delete multiple images 

dangling image --> Image that doesn't have repository and tag 
	docker images --filter "dangling=true" --> lists out just the dangling images 

docker rmi -f $(docker images -q) --> Delete all images which doesnt have a container 

docker system prune 
docker image prune 
	docker image prune -a --> deletes all unused images 
docker container prune 
docker network prune 
docker volume prune 
All the prune commands are cleanup commands 

docker run -d --name <ContainerName> -p hostport:containerPort registery:tag 
	docker run -d javawebapp -p 8080:8080 kedhar5/javawebapp:1 

docker tag --> create a tag that refers to an image 
	docker tag imageID registery/Repository:tag 
	We can tag dangling images using this 
	
docker save <imageTag> -o image.tar --> the docker image can be saved as a tar file 
docker load -i image.tar --> load the image from the tar file 

container commands  
--------------------
Run time instance or a process of an Image is called a container 

docker ps --> lists out the running containers 

docker ps -a --> lists out all types of commands 

docker create --name <createContainer> -P <Hostport:containerport> <image>
	This is for creating a container without starting

docker start <containerId/name> --> start the container 

docker stop <containerID> --> stop the container 

docker restart <containerID> --> restart the container 

docker rm <containerID> --> remove the container 
docker rm -f <containerID> --> forcefully remove the container 

docker create --name <containername> -p <host:container> <image> 
docker run --name <containername> -p <host:container> <image> 
docker container ls --> list all the containers 

docker rm <containerID> --> Container needs to be in stopped state 

docker ps -aq --> gives just the container ID's/w

if we donot mention --name tag in docker run it will give some random name 
So name and Host port should be different for multiple containers 

docker rename <oldname> <new name> --> rename a container 

docker kill <containerID> --> It will kill the process without any grace period 
	Difference b/w kill and stop is stop allows the running transaction until it is completed then stops the conatiner smoothly
	but kill command directly kills the container 

docker pause <containerName> --> Pause the container

docker unpause <containerName> --> Unpause the container 

docker logs <containerName> --> gets the stdout or stderror logs from the job

docker exec <containerName> linux Command --> Linux command will work on the container 
	ex:-- docker exec mavenwebapp ls -lar --> lists out all the folders in the container 

docker exec -it <containername> bash --> interact with the shell of the container 
to come out of the container we can use exit command 

docker top <containerName> --> diaplays the top running process of container 

docker stats <containerName> --> Displays the container stats 

docker cp --> copies files from src to dest 
	docker cp <containerName>:<dest directory> <fileName> 
	docker cp <fileName> <containerName>:<destPath in the Conatiner>

docker commit --> creates a image based on the container 

Dockerfile 
------------
Simple text file which contains instructions build a Docker Image.
Docker Daemon will read the instruction from the file and will build the image accordingly
FROM MAINTAINER COPY ADD CMD RUN ENTRYPOINT LABEL ARG ENV etc.. are some of the instructions 
Docker will process the instructions from top to bottom 

Dockerfile is the default name for Dockerfile 
if we have a specific file we can use -f option 

format of the file is as follows 
INSTRUCTION argument

Instructions are not case sensitive but it is a convention to write them in uppercase 

docker run -d --name ubuntucontainer ubuntu 
	This command will create a image based on ubuntu base image 
	if the image is not available locally it will reach dockerhub by default

FROM instructions gives us the Base image 
	whatever image name we mention in the instruction it will be used as a base image 
	All the following instructions will be run on the same build stage 
	We should only have FROM as the first instruction in Dockerfile 
	Only ARG can appear before Dockerfile
	FROM <image>
	<image> --> Registery/Repository:tag 
	
MAINTAINER is used to set the Author Field of generated image 
	MAINTAINER <name> 
	
LABEL it will add the metadata to the image 
	It is like a key-value pair
	An image can have more than 1 label 
	LABEL <key>=<value>
	LABEL name="kedhar"
	
COPY It copies files/folders from source to destination 
	COPY <src> <dest>
	COPY target/mavenwebapp.war usr/local/tomcat/webapps/mavenwebapp.war 
	src should be relative to the build path
	even if the file exists in the system it should be relative to the build context 
	The difference between COPY and docker cp is COPY copies the file from local to Image 
	docker cp is a command which copies files from system to container and vice versa

ADD It adds files from source to dest 
	ADD can copy local files and folders also it can copy files from remote sources
	We can use URL from the web source tar and zip files can be copied using this 
	tar file will be directly extracted using ADD

RUN executes any command in a new layer on top of current image and then the results will be committed
	RUN will be execited while building the image 
	We can multiple RUN instructions in a Dockerfile 
	We use this to install softwares and configurations while building a image 
	We can also run scripts using this instruction 
	ex: RUN <command> <args>
	RUN apt install git wget curl -y 
	RUN mkdir /opt/test
	We can use shell features it we use shell form like && etc..

The difference between RUN and docker run is RUN is a instruction used to install softwares 
docker run is used to run the container 
	
CMD this is used ro run command or script 
	ex: CMD sh test.sh 
	CMD catalina.sh run 
	RUN , CMD , ENTRYPOINT can run in both shell form and executable form 
	CMD instruction will execute while starting the container
	ex: CMD ["catalina.sh","run"]
	As per the above example catalina.sh script run whenever the container is started 
	We can have numerous number of CMD in Dockerfile but only the last CMD instruction will run while starting the container 
	
	
shellform
-------------
RUN <command> <args>
CMD <command> <args>

Internally the command defined in shell form will be executed as below 
The process will run under a shell as below 
/bin/bash -c <command> <arg1> <arg2> 
So the actual process will be running as a child process 

executable form 
----------------------
RUN ["<executable","<arg1>","<arg2>"]
CMD ["<executable","<arg1>","<arg2>"]

Internally the command defined in executable form will run as below 
<executable> <arg1> <arg2>
ex: apt install wget curl -y 
This be run as the main process or parent process unlike shell form 

The difference is CMD and ENTRYPOINT will start after starting the container 
So for CMD and ENTRYPOINT shell form is not recommended 
So when the process is stopped the main process will be notified about it and can release the resources 
RUN is used while building the image so it will just install the required softwares so need to worry when we stop the process 

Once the Dockerfile is changed and the image is built again then layer gets invalidated and it will be created again
if the same image is going to be created again then the data for building the layers will be retrieved from cache and the image will be created very fast 
Build time can be improved by re-arranging the instructions in the Dockerfile 

ENTRYPOINT will be executed while starting the container 
	CMD is used to start the process 
	ENTRYPOINT will allow you to configure the container to be run as a executable 
	CMD can be overridden at run-time but ENTRYPOINT cannot be overridden
	for example if we use CMD ["test.txt"} at the end of Dockerfile and then run the container the data present will be displayed 
	But if we run as docker run image ls -lar --> ls -lar will be executed and data will be displayed 
	Where as if we have ENTRYPOINT {"test.txt"] then run the conatiner then the data in test file will be displayed 
	but if we use ENTRYPOINT ["test.txt'] ls -lar --> Here too contents of test file will be displayed upon running 
	if we use both CMD and ENTRYPOINT then CMD will be used as argument for entrypoint 
	
ENV is used to set the Environment Variables to the value 
	The value will be in the Variable for all the Docker builds and in the containers 
	ENV <key> <Value> 
	ENV <key>=<path> 
	ex: ENV PROJECT_HOME=/app/test 
	ENV variables can be available after the image is built and the when then container is running
	
ARG instructions is used to set the arguments or variables 
	We can define arguments which user can pass at the build time 
	ARG values are not available after image is bult and a runing container dont have access to the ARG variables 
	ARG is the only instruction which is allowed to be defined before FROM instruction 
	docker build --build-arg <arg=value> -t imagename contextPath
	
WORKDIR is used to set the working directory for an image and a container 
	it is similar to cd command 
	all the instructions after the ORKDIR command will be run in the WORKDIR directory 
	WORKDIR <directory>
	ex: WORKDIR /app/test 
	if $Project_Home=/app/test then 
	WORKDIR $Project_Home 
	If required we can use WORKDIR can be used multiple times in a single Dockerfile
	if the directory doesn't exist then the directory will be created and that will be used as the Working directory 
	
EXPOSE is used to tell on which port the container listens 
	It is not like setting the port it is just for documentation purposes 
	EXPOSE <port>/<protocol>
	EXPOSE 80/udp
	EXPOSE 80/tcp
	It is a documentaion between the person who is building the image and one who is running the container

USER we can set the username or UID as which user the container has to be started 
	This will not create a user it will just set the user 
	if we want to create the create the user before hand we can use the useradd command in the RUN instruction 
	USERADD kedhar -d /app/test -s /bin/bash 
	now our user is created and has access to bash 
	then we can switch the user using USER kedhar 
	Now all the sub sequent instructions will be run as USER kedhar 
	
Best practices while creating Images 
-------------------------------------
1)Use alpine based base images --> Alpine is a Linux distribution and is very light library and size of image is very less 
While puhsing and pulling images it will be very fast by using alpine 
In case of alpine apk is the package manager same as apt for Ubuntu and yum for Linux 
Alpine based image wont have bash shell it just has sh shell 
docker exec -it ContainerName sh --> to execute the commands inside sh shell 

2)Don't install unnecessary s/w or packages in the image while building 

3)Don't copy unnecessary files and folders to your images 

4)in .dockerignore file we can mention the files which need not be in the image 
We can mention those in the file and they will be ignored while building the image 

5)Try to reduce the no.of layers by chaining the commands such that the build time is improved we can use && for chaining 

6)Run the process as a non root user so that someone cant connect to the user shell 

7)Use official images as base images so that we can avoid any malicious activity 

8)try to scan the images before pushing into the artifactory 
	We can identify the security vulnerabilities and find out if any 
	trivy is one such tool used for scanning 
	clair is another scanner for the same purpose 
	https://aquasecurity.github.io/trivy/v0.49/getting-started/installation/
	https://quay.github.io/clair/

9)Use multi stage docker builds to reduce the size of final image 

Python uses pip packaage manager for internal dependencies 
Flask process listens on 5000 port on default 
Flask is a frame work used by python 

Multi Stage Docker Build 
-------------------------
We can use multiple FROM statements in a Docker file 
Each FROM can use a different base and each FROM creates a differnt stage 
We can selectively copy whatever files we want from 1 stage to next 

As an example using maven we can create a war file for Java based project 
in stage 1 we use maven as base we set set WORKDIR and create war file in this stage 
We can just copy the war file from this stage 

In stage 2 we can copy the war file as below 
COPY --from=0 /targer/.war /usr/local/tomcat/.war 
So the war file from stage 0 will be copied to the current stage 
Also Maven local repos and other unnecessary thing will not be copied to next stage 
So overall we can reduce the sie of image 

ECR --> Elastic Container Registery 
--------------------------------------
It is a fully managed registery by AWS 
Docker will work with only secure registeries Registeries need to have SSL certificates 

In ECR we can directly create a repo in the registery and we can pull and push the images easily 
We can setup Registeries by creating servers and using Nexus/JFrog as Artifactory repositories 
In Fully Managed Registeries we need not create the servers etc.. We can just rely on AWS 
We need not worry of availability and scalability of the service 

Using the ECR service we can create a repository and there we can store our artifacts or images 
ECR will eliminate the need to operate and mange the On premise registeries 
ECR transfers images over HTTPS protocols 
ECR can be integrated with other services like EKS Lambda etc..
We pay only for the amount of data we are storing in the repo and data transfer fee 

AES --> ECR --> Get started --> Create Repository --> Public / Private --> repo name 
Tag immutability(On/Off) --> Prevents re-writing the existing image with same tag 
It will avoid the image being pushed and data being rewritten with same tag 

So we have created a repository in ECR now so we can build tag push and pull the images now 
before doing all thet we need to authenticate the AWS with build server 
In order to authenticate with ECR we need a token 
docker login -u AWS -p <token> <ECR end point> 
AWS gives us the steps to Authenticate with ECR too 

Before doing authentication we need to install AWS CLI in the Docker system 
sudo apt install awscli --> AWS CLI will be installed 
Even after installling AWS CLI we dont have access to AWS services on Ubuntu system 

We can create a IAM role and we can attach the role
iAM --> Create role --> AWS service --> EC2 --> Policies( ContainerRegistery full / read only access) --> Name --> create role 
If we give read only policy we can pull but cant push the images 
Now role is created we need to attach the role to EC2 as following 
Select the build server --> Action --> security --> Add role 
Now the security issue is sorted 
Now we can authenticate the login and push the image to ECR 

For the deployment server create another IAM role with container read only access and attch it to the EC2 deployment server 
Now we can authenticate the credentials in EC2 
AWS ECR authentication is valid for just 12 hours 

Monolithic & Miicro Services Architecture 
------------------------------------------
These are not Docker concepts they are developmental approach 

Monolithic --> Single Code base, All the modules in the same base or SCM tool 
GUI API and database all the code will be present in the same code base
It has a single executable or deployable package 
Advantages 
-----------
1. Easy to deploy 
2. Easy to trace the errors and debugging is easy too 
Disadvantages 
--------------
1. difficult to modify the source code 
2. All the modules are tightly coupled 
3. redeploy entire app with each update 
4. Single bug can bring down the app 
5. Scaling the app is difficult 
6.Change in 1 module can change other modules too because they are tightly coupled 

Microservices --> developmental technique which arranges an app as a collection of loosely coupled services 
here each module can be maintained individually
Each microservice performs specific task and communicates using API's 
Advantages 
-----------
1. Each service is looesly coupled 
2. Easy to understand & modify 
3. Better deployments because of individual modules 
4. Each module can be scaled independently 
5. Each service can be developed using any new technology 

Disadvantages
--------------
1. communication between service is difficult 
	since everything is independent using of API's for every communication will be difficult 
2. Integration testing is difficult 
3. debugging is difficult 
4. Deployment challenges --> app needs coordination among multiple services it is not straight forward 

Docker Networks 
--------------------
By default there are three types of networks They are 
1. Bridge 
2. Host 
3. none 

Bridge 
-------
It is default network for a docker container 
If the conatiners are on same network the inter container communication is possible using container Ip and port number 
We cannot contact other containers using container name 
Container IP's are dynamic and name will be constant 
We can create own our own private docker network as follows 
	docker network create -d bridge <networkName> 
We can use the network to create containers as follows 
	docker run -d --name <containerName> -p host:container --network <NetworkName> <image> 
Containers built on diffrent networks cannot communicate between each other 
If we use a custom network we can contact containers of that network using the container name instead of IP 
A single container can be attached to multiple networks as follows 
	docker network connect <NetworkName> <ContainerName> 

Host Network 
-------------
If we create our container here there will be no container IP 
There wont be any isolation between containers 
We can run the containers without port binding 
Two containers cannot listen on the same port here 

None Network 
-------------
If we create our container here there won't be any IP too.

Scope of bridge network is local so we use overlay network for communication between containers in different servers 

Docker Volumes
---------------
Every container has it's own file system and the file system exists as long as the container exists 
Statefull Application --> Application which is going to generate some data and stores and uses it 
ex :-- databases like MySql Oracle Mongo 
Stateless Application --> application which doesnot generate and store any data 

We need to check if the Database is up and running for Statefull applications 
Database can be anywhere and databases will be also available as a container 

Docker 17 yet to be written 

Once we mount the file or folder all the data will be present in the container will be present in the bind mount folder in Host too 

Volumes 
---------
Volumes are the preferred mechanism for storing the data for Docker 
Volumes are completely managed by docker so has many advantages over Bind mounts 
Voulmes are easy to backup or easy to be migrated 
Volumes work on both Windows and Linux containers 
Volume drivers will allow remote machines to make changes to volumes 
When we are using volumes a new directory will be created in Docker container and it will be managed by Docker 

docker volume ls --> lists out the volumes in the docker system 

Named Volumes 
--------------
Volumes which can be reused while creating a new container using the existing data 
Volume details will be managed by Docker itself and within docker 

docker volume create -d <driver> <volumeName> --> a new volume will be created 
By default we have only local driver 
docker volume create local mongovol --> voume called mongovol will be created 

to attach the volume 
docker run -d --name <contName> -v volumeName:FolderInContainer  <image> 
docker run -d --name <contName> -v volumeName:/data/db  <image> 

By installing plugins/drivers we can create volumes on Azure/AWS cloud 

We can install ebs drivers so that ebs volumes can be monuted with the container 
docker volume create ebs <VolumeName> --> uses a remote driver 

Docker Compose 
---------------
Notes need to be written here 

Compose software needs to be installed in the Docker system 
sudo apt install docker-compose --> install compose 
compose will expect compose file with default name in the current working directory 
If the compose file is not presnt in the directory then an error will be thrown 
if we have a diffent name compose file we can use as follows 
docker-compose -f docker-app.yaml config --> Checks the compose file for any errors 

docker-compose compose.yml config --> Checks the configurations of the compose file 
docker-compose create --> Creates network volume services 
docker-compose up -d --> creates and starts the conatiners volumes networks 
docker-compose stop --> stops the containers which are created by the compose file 
docker-compose start --> starts the container which are in the compose file 
docker-compose down --> deletes the containers,networks within scope of compose file 
